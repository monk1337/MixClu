{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score, adjusted_mutual_info_score, adjusted_rand_score, calinski_harabasz_score, davies_bouldin_score\n",
    "# from sklearn.neighbors import DistanceMetric\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# from kmodes.kprototypes import KPrototypes\n",
    "# from kmodes.kmodes import KModes\n",
    "# from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# import umap\n",
    "\n",
    "# import io\n",
    "# import math\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# from community import community_louvain\n",
    "# import networkx as nx\n",
    "\n",
    "# from matplotlib import gridspec\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from mixclu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(scores,  preds, labels, name='', X=None):\n",
    "    \n",
    "    if X is not None:\n",
    "\n",
    "        silhouette = silhouette_score(X, preds, metric='euclidean')\n",
    "        cal_har = calinski_harabasz_score(X, preds)\n",
    "        dav_bould = davies_bouldin_score(X, preds)\n",
    "\n",
    "        adj_mut_info = adjusted_mutual_info_score(labels, preds, average_method='arithmetic')\n",
    "        adj_rand = adjusted_rand_score(labels, preds)\n",
    "\n",
    "        content = {'Algorithm':name,\n",
    "                   'Silhouette':silhouette,\n",
    "                   'Calinski_Harabasz':cal_har,\n",
    "                   'Davis Bouldin':dav_bould,\n",
    "                   'Adjusted_Mutual_Info':adj_mut_info,\n",
    "                   'Adjusted_Rand_Score':adj_rand}\n",
    "\n",
    "        scores = scores.append(content, ignore_index = True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        adj_mut_info = adjusted_mutual_info_score(labels, preds, average_method='arithmetic')\n",
    "        adj_rand = adjusted_rand_score(labels, preds)\n",
    "\n",
    "        content = {'Algorithm':name,\n",
    "                   'Silhouette':np.NaN,\n",
    "                   'Calinski_Harabasz':np.NaN,\n",
    "                   'Davis Bouldin':np.NaN,\n",
    "                   'Adjusted_Mutual_Info':adj_mut_info,\n",
    "                   'Adjusted_Rand_Score':adj_rand}\n",
    "\n",
    "        scores = scores.append(content, ignore_index = True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gower_distance(df, cat_columns):\n",
    "    if cat_columns:\n",
    "        variable_distances = gower.gower_matrix(df,cat_features= \n",
    "                           [True if df[k].dtypes == np.object else False \n",
    "                            for k in df.columns])\n",
    "    else:\n",
    "        variable_distances = gower.gower_matrix(df)\n",
    "    \n",
    "    variable_distances[np.isnan(variable_distances)] = 0\n",
    "    return variable_distances\n",
    "\n",
    "\n",
    "def calculate_zscore(df, columns):\n",
    "    '''\n",
    "    scales columns in dataframe using z-score\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        df[col] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(df, columns):\n",
    "    \n",
    "    '''\n",
    "    one hot encodes list of columns and\n",
    "    concatenates them to the original df\n",
    "    '''\n",
    "\n",
    "    concat_df = pd.concat([pd.get_dummies(df[col], drop_first=True, prefix=col) for col in columns], axis=1)\n",
    "    one_hot_cols = concat_df.columns\n",
    "\n",
    "    return concat_df, one_hot_cols\n",
    "\n",
    "\n",
    "\n",
    "def normalize_column_modality(df, columns):\n",
    "    '''\n",
    "    divides each column by the probability μₘ of the modality \n",
    "    (number of ones in the column divided by N) only for one hot columns\n",
    "    '''\n",
    "\n",
    "    length = len(df)\n",
    "    for col in columns:\n",
    "\n",
    "        weight = math.sqrt(sum(df[col])/length)\n",
    "        df[col] = df[col]/weight\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def center_columns(df, columns):\n",
    "    '''\n",
    "    center columns by subtracting the mean value\n",
    "    '''\n",
    "    for col in columns:\n",
    "        df[col] = (df[col] - df[col].mean())\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def FAMD_2(df, n_components=2):\n",
    "    '''\n",
    "    Factorial Analysis of Mixed Data (FAMD), \n",
    "    which generalizes the Principal Component Analysis (PCA) \n",
    "    algorithm to datasets containing numerical and categorical variables\n",
    "\n",
    "    a) For the numerical variables\n",
    "    - Standard scale (= get the z-score)\n",
    "\n",
    "    b) For the categorical variables:\n",
    "    - Get the one-hot encoded columns\n",
    "    - Divide each column by the square root of its probability sqrt(μₘ)\n",
    "    - Center the columns\n",
    "\n",
    "    c) Apply a PCA algorithm over the table obtained!\n",
    "\n",
    "    '''\n",
    "\n",
    "    variable_distances = []\n",
    "    numeric_cols = data.select_dtypes(include=np.number)\n",
    "    cat_cols = data.select_dtypes(include='object')\n",
    "\n",
    "    # numeric process\n",
    "    normalized_df = calculate_zscore(df, numeric_cols)\n",
    "    normalized_df = normalized_df[numeric_cols.columns]\n",
    "\n",
    "    # categorical process\n",
    "    cat_one_hot_df, one_hot_cols = one_hot_encode(df, cat_cols)\n",
    "    cat_one_hot_norm_df = normalize_column_modality(cat_one_hot_df, one_hot_cols)\n",
    "    cat_one_hot_norm_center_df = center_columns(cat_one_hot_norm_df, one_hot_cols)\n",
    "\n",
    "    # Merge DataFrames\n",
    "    processed_df = pd.concat([normalized_df, cat_one_hot_norm_center_df], axis=1)\n",
    "\n",
    "    # Perform (PCA)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(processed_df)\n",
    "\n",
    "    return principalComponents\n",
    "\n",
    "\n",
    "# use Umap to do embedding then cluster on that\n",
    "def umap_reduce(df, intersection=False):\n",
    "    \n",
    "    numerical = df.select_dtypes(exclude='object')\n",
    "    for c in numerical.columns:\n",
    "        numerical[c] = (numerical[c] - numerical[c].mean())/numerical[c].std(ddof=0)\n",
    "      \n",
    "    ##preprocessing categorical\n",
    "    categorical = df.select_dtypes(include='object')\n",
    "    categorical = pd.get_dummies(categorical)\n",
    "\n",
    "\n",
    "\n",
    "    #Embedding numerical & categorical\n",
    "    fit1 = umap.UMAP(random_state=12).fit(numerical)\n",
    "    fit2 = umap.UMAP(metric='dice', n_neighbors=250).fit(categorical)\n",
    "\n",
    "    numeric_embedding = fit1.embedding_\n",
    "    numeric = pd.DataFrame(\n",
    "                         {'x': numeric_embedding[:,0],\n",
    "                         'y':  numeric_embedding[:,1],\n",
    "                        })\n",
    "\n",
    "\n",
    "    categorical_embedding = fit2.embedding_\n",
    "    categorical = pd.DataFrame(\n",
    "                         {'x': categorical_embedding[:,0],\n",
    "                         'y':  categorical_embedding[:,1],\n",
    "                        })\n",
    "\n",
    "\n",
    "    # intersection will resemble the numerical embedding more.\n",
    "    if intersection:\n",
    "        embedding = fit1 * fit2\n",
    "\n",
    "    # union will resemble the categorical embedding more.\n",
    "    else:\n",
    "        embedding = fit1 + fit2\n",
    "\n",
    "    umap_embedding = embedding.embedding_\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "                        {'x': umap_embedding[:,0],\n",
    "                         'y':  umap_embedding[:,1],\n",
    "                        })\n",
    "    \n",
    "    return results, umap_embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def elbow_method_kmeans(df, space=(2,11)):\n",
    "\n",
    "    cost = []\n",
    "    n_clusters = []\n",
    "\n",
    "    start = space[0]\n",
    "    stop  = space[1]\n",
    "    for k in range(start, stop):\n",
    "        kmeans = KMeans(n_clusters=k, verbose=0)\n",
    "        kmeans.fit(df)\n",
    "        cost.append(kmeans.inertia_)\n",
    "        n_clusters.append(k)\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "                        {'n_clusters': n_clusters,\n",
    "                        'cost': cost,\n",
    "                        })\n",
    "  \n",
    "    return (p9.ggplot(results, p9.aes(x='n_clusters', y='cost'))\n",
    "            + p9.geom_point()\n",
    "            + p9.geom_line()\n",
    "            + p9.ggtitle('Elbow Plot'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_knn_bins(df, cols, \n",
    "                 bins=5, \n",
    "                 drop_cols=True, \n",
    "                 encode = True):\n",
    "    \n",
    "    k_columns = []\n",
    "    \n",
    "    for col in cols:\n",
    "    \n",
    "        kmeans  = KMeans(n_clusters=bins).fit(df[col].to_frame().values.reshape(-1,1))\n",
    "        results = pd.DataFrame(kmeans.labels_, columns=[col + '_centroid'])\n",
    "\n",
    "        df = df.reset_index()\n",
    "        df[col + '_centroid'] = results[col + '_centroid']\n",
    "\n",
    "        knn_bin_df = pd.DataFrame(kmeans.cluster_centers_)\n",
    "        knn_bin_df = knn_bin_df.astype(int).reset_index()\n",
    "\n",
    "        temp_df = pd.merge(df[col + '_centroid'],\n",
    "                           knn_bin_df, \n",
    "                           left_on=col + '_centroid',\n",
    "                           right_on='index',\n",
    "                           how='left')\n",
    "\n",
    "        # rename empty column header 0 -> column_name value\n",
    "        temp_df = temp_df.rename(columns={0:col+'_value'})\n",
    "\n",
    "        temp_df.loc[:,col+'_value'] = col + '_' + temp_df[col+'_value'].astype(str)\n",
    "\n",
    "        df = pd.concat([df, temp_df[col+'_value']], axis=1)\n",
    "        df.drop([col + '_centroid', 'index'], axis=1, inplace=True)\n",
    "        k_columns.append(col+'_value')\n",
    "    \n",
    "    cat_columns = [k for k in df.columns if k not in cols]\n",
    "    print(\"New cat columns \", \",\".join(k_columns))\n",
    "    df = cobj(df, cat_columns)\n",
    "    \n",
    "    if drop_cols:\n",
    "        df = df.drop(cols, axis=1)\n",
    "    \n",
    "    if encode:\n",
    "        df = encode_columns(df, k_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def convert_df_to_sgraph_network(df):\n",
    "    '''\n",
    "    This function converts a dataframe into an edge list and finally\n",
    "    into a network graph\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    edges_df = pd.DataFrame()\n",
    "    # create a name for each row\n",
    "    length = len(df)\n",
    "    row_names = ['row '+ str(i) for i in range(1, length+1)]\n",
    "\n",
    "    original_cols = df.columns\n",
    "    df['row_name'] = row_names\n",
    "\n",
    "    for col in original_cols:\n",
    "        col_edge_df = df[['row_name', col]].rename(columns={col:'to'})\n",
    "        edges_df = pd.concat([edges_df, col_edge_df], axis=0)\n",
    "\n",
    "    # set the edge weights to one\n",
    "    edges_df['weight'] = 1\n",
    "    edges_df = edges_df.groupby(['row_name', 'to']).count().reset_index()\n",
    "    edges_df.rename(columns={'row_name':'from'}, inplace=True)\n",
    "\n",
    "    graph = nx.from_pandas_edgelist(edges_df, source='from',\n",
    "                                  target='to', edge_attr=['weight'])\n",
    "  \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cat_columns, con_feats, y = get_dummy_data(50, \n",
    "                                               8, \n",
    "                                               7,\n",
    "                                               centers        = 3,\n",
    "                                               missing_values = None, \n",
    "                                               id_cols        = None)\n",
    "df = cobj(df, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "\n",
    "def gower_distance(X):\n",
    "    \"\"\"\n",
    "    This function expects a pandas dataframe as input\n",
    "    The data frame is to contain the features along the columns. Based on these features a\n",
    "    distance matrix will be returned which will contain the pairwise gower distance between the rows\n",
    "    All variables of object type will be treated as nominal variables and the others will be treated as \n",
    "    numeric variables.\n",
    "    Distance metrics used for:\n",
    "    Nominal variables: Dice distance (https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "    Numeric variables: Manhattan distance normalized by the range of the variable (https://en.wikipedia.org/wiki/Taxicab_geometry)\n",
    "    \"\"\"\n",
    "    individual_variable_distances = []\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        feature = X.iloc[:,[i]]\n",
    "        if feature.dtypes[0] == np.object:\n",
    "            feature_dist = DistanceMetric.get_metric('dice').pairwise(pd.get_dummies(feature))\n",
    "        else:\n",
    "            feature_dist = DistanceMetric.get_metric('manhattan').pairwise(feature) / max(np.ptp(feature.values),1)\n",
    "\n",
    "        individual_variable_distances.append(feature_dist)\n",
    "\n",
    "    return np.array(individual_variable_distances).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_gower_distance(df, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd=pd.DataFrame({'age':[21,21,19, 30,21,21,19,30,None],\n",
    "'gender':['M','M','N','M','F','F','F','F',None],\n",
    "'civil_status':['MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED',None],\n",
    "'salary':[3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0,None],\n",
    "'has_children':[1,0,1,1,1,0,0,1,None],\n",
    "'available_credit':[2200,100,22000,1100,2000,100,6000,2200,None]})\n",
    "\n",
    "calculate_gower_distance(Xd, cat_columns = [True, True, True, False, True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in df.columns:\n",
    "    print(df[k].dtypes == np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "X = np.asarray(df)\n",
    "gower.gower_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gower\n",
    "gower.gower_matrix(df,cat_features= [True if df[k].dtypes == np.object else False for k in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "df = pd.DataFrame([[1,2.6,'A'],[12,5,'X'],[4,7,'A']])\n",
    "df.columns = ['Num_1','Num_2','Cat_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df.head(5))\n",
    "\n",
    "[True if df[k].dtypes == np.object else False for k in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = DistanceMetric.get_metric('manhattan').pairwise(df[['Num_1']])\n",
    "s1 = s1/max(np.ptp(df['Num_1']),1)\n",
    "s2 = DistanceMetric.get_metric('manhattan').pairwise(df[['Num_2']])/max(np.ptp(df['Num_2']),1)\n",
    "s3 = DistanceMetric.get_metric('dice').pairwise(dummy_df)\n",
    "Gowers_Distance = (s1*w1 + s2*w2 + s3*w3)/(w1 + w2 + w3) \n",
    "Gowers_Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = DistanceMetric.get_metric('dice').pairwise(pd.get_dummies(df['Cat_1'], drop_first=True))\n",
    "s3[np.isnan(s3)] = 0\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w2 = w3 = 1\n",
    "Gowers_Distance = (s1*w1 + s2*w2 + s3*w3)/(w1 + w2 + w3) \n",
    "Gowers_Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "gower.gower_matrix(df,cat_features= [False, True, False, True, False, False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_gower_distance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_bins(df, cols, \n",
    "                 bins=5, \n",
    "                 drop_cols=True, \n",
    "                 encode = True):\n",
    "    \n",
    "    k_columns = []\n",
    "    \n",
    "    for col in cols:\n",
    "    \n",
    "        kmeans  = KMeans(n_clusters=bins).fit(df[col].to_frame().values.reshape(-1,1))\n",
    "        results = pd.DataFrame(kmeans.labels_, columns=[col + '_centroid'])\n",
    "\n",
    "        df = df.reset_index()\n",
    "        df[col + '_centroid'] = results[col + '_centroid']\n",
    "\n",
    "        knn_bin_df = pd.DataFrame(kmeans.cluster_centers_)\n",
    "        knn_bin_df = knn_bin_df.astype(int).reset_index()\n",
    "\n",
    "        temp_df = pd.merge(df[col + '_centroid'],\n",
    "                           knn_bin_df, \n",
    "                           left_on=col + '_centroid',\n",
    "                           right_on='index',\n",
    "                           how='left')\n",
    "\n",
    "        # rename empty column header 0 -> column_name value\n",
    "        temp_df = temp_df.rename(columns={0:col+'_value'})\n",
    "\n",
    "        temp_df.loc[:,col+'_value'] = col + '_' + temp_df[col+'_value'].astype(str)\n",
    "\n",
    "        df = pd.concat([df, temp_df[col+'_value']], axis=1)\n",
    "        df.drop([col + '_centroid', 'index'], axis=1, inplace=True)\n",
    "        k_columns.append(col+'_value')\n",
    "    \n",
    "    cat_columns = [k for k in df.columns if k not in cols]\n",
    "    print(\"New cat columns \", \",\".join(k_columns))\n",
    "    df = cobj(df, cat_columns)\n",
    "    \n",
    "    if drop_cols:\n",
    "        df = df.drop(cols, axis=1)\n",
    "    \n",
    "    if encode:\n",
    "        df = encode_columns(df, k_columns)\n",
    "\n",
    "    return df\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "df, cat_columns, con_feats, y = get_dummy_data(50, \n",
    "                                               8, \n",
    "                                               3,\n",
    "                                               centers        = 3,\n",
    "                                               missing_values = None, \n",
    "                                               id_cols        = None)\n",
    "df = cobj(df, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = get_knn_bins(df, con_feats, bins=5, drop_cols = True, encode = True)\n",
    "rt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_df_to_sgraph_network(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt['X1_value'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n",
    "\n",
    "# ‘uniform’, ‘quantile’, ‘kmeans’\n",
    "# print(est.fit_transform(df))\n",
    "dfrt = pd.DataFrame(est.fit_transform(df[con_feats]))\n",
    "dfrt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = u\"Hello, Welcome\\nPlease follow the instructions \\u2191\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Welcome\\nPlease follow the instructions ↑'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
